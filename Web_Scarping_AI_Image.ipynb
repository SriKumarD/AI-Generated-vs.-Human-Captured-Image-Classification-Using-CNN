{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Web Scraping AI Images**\n",
        "---\n",
        "Importing Required Libraries\n",
        "We start by importing the necessary libraries:\n",
        "- `selenium`: For automating the Chrome browser.\n",
        "- `requests`: To download images.\n",
        "- `beautifulsoup4`: For parsing HTML and extracting data from web pages.\n",
        "- `time`: To introduce delays during the scraping process.\n",
        "\n",
        "In the following sections, Will delve into the web scraping process to collect AI-generated images from specified sources using Selenium, BeautifulSoup, and requests.\n",
        "\n",
        "---\n",
        "**Running the Web Scraping Code**\n",
        "\n",
        "To run the web scraping code for AI image collection follow these instructions:\n",
        "\n",
        "1. **Prerequisites**:\n",
        "   - Ensure you have Python installed on your local machine.\n",
        "   - Install the necessary Python libraries if you haven't already. You can use `pip` to install them:\n",
        "     ```python\n",
        "     pip install selenium\n",
        "     ```\n",
        "      ```python\n",
        "     pip install requests\n",
        "     ```\n",
        "      ```python\n",
        "     pip install beautifulsoup4\n",
        "     ```\n",
        "\n",
        "2. **WebDriver Setup**:\n",
        "   - Download the appropriate WebDriver for your browser. In this code, we are using the Chrome WebDriver. Make sure it matches your Chrome browser version.\n",
        "   - Download the Chrome WebDriver from [ChromeDriver Downloads](https://sites.google.com/chromium.org/driver/).\n",
        "   - Place the WebDriver executable in a directory that's included in your system's PATH.\n",
        "\n",
        "\n",
        "Ensure that you have the required WebDriver installed and set up correctly on your local machine to execute the web scraping code successfully.\n"
      ],
      "metadata": {
        "id": "fFc-Ml_88VZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code For Web Scraping AI Images from Gencraft**"
      ],
      "metadata": {
        "id": "wGOPSHv_9JdV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import time  # Import the 'time' library for managing time-related operations\n",
        "from selenium import webdriver  # Import the Selenium library for web automation\n",
        "from selenium.webdriver.common.by import By  # Import 'By' class for selecting elements by different methods\n",
        "from selenium.webdriver.support.ui import WebDriverWait  # Import 'WebDriverWait' for waiting for elements\n",
        "from selenium.webdriver.support import expected_conditions as EC  # Import 'expected_conditions' for specifying conditions\n",
        "import requests  # Import the 'requests' library for HTTP requests\n",
        "from PIL import Image  # Import the 'Image' class from the 'PIL' library for image processing\n",
        "import imagehash  # Import the 'imagehash' library for image hashing\n",
        "import io  # Import the 'io' module for input/output operations\n",
        "\n",
        "# Create a new instance of Google Chrome\n",
        "driver = webdriver.Chrome()  # Initialize a new Chrome browser instance\n",
        "\n",
        "# Open the website in the Chrome browser\n",
        "driver.get(\"https://gencraft.com/explore\")  # Navigate to the specified website\n",
        "\n",
        "# Define the target class name\n",
        "target_class = \"w-full h-full bg-blue-200\"  # Define the class name to locate specific images\n",
        "\n",
        "# Scroll down to load images and download up to 1000 unique images\n",
        "downloaded_images = 0  # Initialize a counter for downloaded images\n",
        "unique_image_hashes = set()  # Create a set to store unique image hashes\n",
        "\n",
        "while downloaded_images < 1000:  # Execute the following block until 1000 images are downloaded\n",
        "    # Scroll down\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")  # Scroll down the webpage\n",
        "    time.sleep(2)  # Wait for 2 seconds to let the images load\n",
        "\n",
        "    # Find all img elements with the target class\n",
        "    img_elements = driver.find_elements(By.XPATH, f'//img[contains(@class, \"{target_class}\")]')  # Find specific images\n",
        "\n",
        "    # Iterate through the img elements and download the images\n",
        "    for img_element in img_elements:  # Loop through the found image elements\n",
        "        src = img_element.get_attribute(\"src\")  # Get the image source URL\n",
        "        if src:\n",
        "            response = requests.get(src)  # Send an HTTP GET request to the image URL\n",
        "            if response.status_code == 200:  # If the request is successful (HTTP 200 OK)\n",
        "                image = Image.open(io.BytesIO(response.content))  # Open the image using PIL\n",
        "                hash = str(imagehash.average_hash(image))  # Compute the average hash of the image\n",
        "\n",
        "                # Check if the image is unique\n",
        "                if hash not in unique_image_hashes:  # If the hash is not in the set of unique hashes\n",
        "                    with open(f\"AI_Generated/image_{downloaded_images}.jpg\", \"wb\") as file:\n",
        "                        file.write(response.content)  # Save the image to a file\n",
        "                    unique_image_hashes.add(hash)  # Add the image hash to the set of unique hashes\n",
        "                    downloaded_images += 1  # Increment the downloaded image counter\n",
        "                    if downloaded_images >= 1000:  # If 1000 images are downloaded, exit the loop\n",
        "                        break\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()  # Terminate the Chrome browser instance\n"
      ],
      "metadata": {
        "id": "LWQyMp2o8mo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code For Web Scraping AI Images from Pixabay**"
      ],
      "metadata": {
        "id": "kriPy-399TlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import time  # Import the 'time' library for managing time-related operations\n",
        "from selenium import webdriver  # Import the Selenium library for web automation\n",
        "from selenium.webdriver.common.by import By  # Import 'By' class for selecting elements by different methods\n",
        "import requests  # Import the 'requests' library for HTTP requests\n",
        "\n",
        "# Create a new instance of Google Chrome\n",
        "driver = webdriver.Chrome()  # Initialize a new Chrome browser instance\n",
        "\n",
        "# Set the initial page number and downloaded image count\n",
        "page_number = 436\n",
        "downloaded_images = 8523\n",
        "\n",
        "while downloaded_images < 50000:  # Execute the loop until 50,000 images are downloaded\n",
        "    # Open the website with the current page number\n",
        "    url = f\"https://pixabay.com/images/search/ai%20generated/?pagi={page_number}\"\n",
        "    driver.get(url)  # Navigate to the specified URL\n",
        "\n",
        "    # Initialize the previous image count\n",
        "    prev_image_count = 0\n",
        "\n",
        "    # Scroll down repeatedly until new images stop loading\n",
        "    while True:\n",
        "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")  # Scroll down the webpage\n",
        "        time.sleep(2)  # Wait for 2 seconds to let new images load\n",
        "\n",
        "        # Find all anchor elements with the specified class\n",
        "        anchor_elements = driver.find_elements(By.XPATH, '//a[@class=\"link--WHWzm\"]')  # Find specific anchor elements\n",
        "\n",
        "        # Check if new images have loaded\n",
        "        curr_image_count = len(anchor_elements)\n",
        "        if curr_image_count == prev_image_count:\n",
        "            break\n",
        "\n",
        "        prev_image_count = curr_image_count\n",
        "\n",
        "    # Iterate through the anchor elements and download the images\n",
        "    for anchor_element in anchor_elements:  # Loop through the found anchor elements\n",
        "        img_element = anchor_element.find_element(By.TAG_NAME, 'img')  # Find the image element within the anchor\n",
        "        src = img_element.get_attribute(\"src\")  # Get the image source URL\n",
        "        if src:\n",
        "            response = requests.get(src)  # Send an HTTP GET request to the image URL\n",
        "            if response.status_code == 200:  # If the request is successful (HTTP 200 OK)\n",
        "                with open(f\"AI_50000/image_{downloaded_images}.jpg\", \"wb\") as file:\n",
        "                    file.write(response.content)  # Save the image to a file\n",
        "                downloaded_images += 1\n",
        "\n",
        "    # Increment the page number for the next iteration\n",
        "    page_number += 1\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()  # Terminate the Chrome browser instance\n"
      ],
      "metadata": {
        "id": "fy4C78jP9TyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Code for Web Scraping AI Images from DeviantArt for Validation Set**"
      ],
      "metadata": {
        "id": "rRhEXm5w9T8e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import time  # Import the 'time' library for managing time-related operations\n",
        "from selenium import webdriver  # Import the Selenium library for web automation\n",
        "from selenium.webdriver.common.by import By  # Import 'By' class for selecting elements by different methods\n",
        "import requests  # Import the 'requests' library for HTTP requests\n",
        "\n",
        "# Create a new instance of Google Chrome\n",
        "driver = webdriver.Chrome()  # Initialize a new Chrome browser instance\n",
        "\n",
        "# Set the initial page number and downloaded image count\n",
        "page_number = 1\n",
        "downloaded_images = 0\n",
        "\n",
        "while page_number < 156:  # Execute the loop until page 156\n",
        "    # Open the website with the current page number\n",
        "    url = f\"https://www.deviantart.com/sono2000/gallery?page={page_number}\"\n",
        "    driver.get(url)  # Navigate to the specified URL\n",
        "\n",
        "    # Scroll down to load images and wait for images to load\n",
        "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\")  # Scroll down the webpage\n",
        "    time.sleep(2)  # Wait for 2 seconds to let new images load\n",
        "\n",
        "    # Find all image elements within the specified div structure\n",
        "    img_elements = driver.find_elements(By.XPATH, '//div[@data-testid=\"thumb\"]/img')  # Find specific image elements\n",
        "\n",
        "    # Iterate through the image elements and download the images\n",
        "    for img_element in img_elements:  # Loop through the found image elements\n",
        "        src = img_element.get_attribute(\"src\")  # Get the image source URL\n",
        "        if src:\n",
        "            response = requests.get(src)  # Send an HTTP GET request to the image URL\n",
        "            if response.status_code == 200:  # If the request is successful (HTTP 200 OK)\n",
        "                with open(f\"AI_Train/image_{downloaded_images}.jpg\", \"wb\") as file:\n",
        "                    file.write(response.content)  # Save the image to a file\n",
        "                downloaded_images += 1\n",
        "\n",
        "    # Increment the page number for the next iteration\n",
        "    page_number += 1\n",
        "\n",
        "# Close the browser\n",
        "driver.quit()  # Terminate the Chrome browser instance\n"
      ],
      "metadata": {
        "id": "V-7fo0Yx9UHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "In this notebook, we've explored a series of web scraping code snippets that serve the purpose of collecting AI-generated images from various online sources. Web scraping is a powerful technique for data collection, and in this context, it has been used to create datasets for AI image classification projects.\n",
        "\n",
        "We began by introducing the code for web scraping AI images from [GenCraft](https://gencraft.com/explore), followed by a code snippet for gathering AI images from [Pixabay](https://pixabay.com/images/search/ai%20generated/). Both of these code sections make use of the Selenium library to automate the web browsing process, locate, and download images.\n",
        "\n",
        "We further extended the capabilities of web scraping by introducing a code snippet for collecting AI images from [DeviantArt](https://www.deviantart.com/sono2000/gallery). This section showcases how to navigate through multiple pages, extract image elements, and store the downloaded images locally.\n",
        "\n",
        "The collected data from these web scraping activities has been utilized for an AI-generated and human-captured image classification project. We employed Convolutional Neural Networks (CNN) for this purpose, using the scraped data to train and evaluate our models.\n",
        "\n",
        "To access the full details of the AI-generated and human-captured image classification project using CNN, you can follow this [link](https://colab.research.google.com/drive/1gSk-zoGXZ15lRTYeTAw8VL12JR4uRkkZ?usp=share_link), which provides a comprehensive overview of the project, including the model architecture, dataset preparation, and evaluation metrics.\n",
        "\n",
        "These code snippets can be further customized and integrated into larger machine learning projects to train and evaluate image classification models.\n",
        "\n",
        "**Remember that web scraping should always be conducted in accordance with the terms of use and policies of the websites you are scraping. It's important to be respectful of website owners and their content.**\n",
        "\n",
        "*Happy web scraping, Deep learning, and image classification!*\n"
      ],
      "metadata": {
        "id": "2NLc29BaDT5Q"
      }
    }
  ]
}